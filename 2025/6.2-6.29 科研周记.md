## 6.2-6.29 科研周记

### 第一周（6.2-6.8）

1. 精读了一篇文献：《Longitudinal dementia trajectories for Alzheimer’s Disease characterization and prediction》，其核心工作主要是构建了一个纵向阿尔兹海默症预测模型——LgAD，它的思路是融合多模态数据，通过CatBoost模型输出连续型、范围为0-1的SDS值，然后利用贝叶斯非线性混合效应模型对SDS值进行纵向轨迹建模，从而建模不同类别患者的患病轨迹，既能实现预测，又能描述疾病发展过程。

   学习到了“通过贝叶斯非线性混合效应模型构建纵向轨迹”这一研究思路。


### 第二周（6.9-6.15）

1. 学习了《广义线性模型》的零膨胀模型，其基本原理是固定零部分用logistic模型建模，计数部分用poisson回归建模， 它区分"结构性零" 和 "随机零" 两种来源；

1. 学习了《广义线性模型》的Hurdle回归，它也是一种处理过度零值数据的统计方法，但它不考虑“随机零”部分，使用截断计数分布对计数部分进行建模；

1. 对阈值最小二乘法有了新的思路，可以考虑“截断正态分布”；

1. 学习了效应修饰和混杂因素的区别，效应修饰是真实的，它也叫交互效应，而混杂因素是虚假的，混杂因素的定义是：第三个变量（混杂因子）同时与暴露因素和结局相关，且并非暴露到结局的中间变量，从而歪曲了暴露与结局的真实关联。这两者**本质是完全独立的概念**，**“变量与暴露 / 结局的相关性”≠“暴露效应在变量分层中的差异”**；

1. 学习到了平方和展开公式：
   $$
   (\sum_{i=1}^{n}{X_i})^2=\sum_{i=1}^{n}{X_i^2}+2\sum_{i<j}^{n}{X_iX_j}
   $$

1. 学完了概率论第七章《期望的性质》，学习到了独立正态变量的线性组合的==联合分布==也是多元正态分布，学到了两个条件期望相关公式：$E(X) = E(E(X|Y))$，$Var(X)=E(Var(X|Y))+Var(E(X|Y))$，学习到了矩母函数的性质：$M^{(n)}_X(0)=E(X^n)$，即矩母函数的 $n$ 阶导在 $t=0$ 处的值为随机变量的 $n$ 阶原点矩；

1. 学习了IRLS，它是一种优化算法，而不是估计方法，本质上是牛顿迭代法在MLE中的具体实现。

### 第三周（6.16-6.22）

1. 学习了probit模型，它的连接函数是$\Phi^{-1}(\cdot)$，它适用于低于某种阈值时，事件发生的二项分布GLM模型，具体推导为：

   假设存在某种潜在变量 $t_i$，它的形式为：
   $$
   t_i = x_i'\delta + \varepsilon_i,\quad \varepsilon_i \sim N(0,\sigma^2)\\
   $$
   当 $t_i$ 小于某个阈值时发生观测事件，即：$y_i = I(t_i \leq T)$，此时连接函数应该用逆正态分布函数：
   $$
   \begin{align*} P(y_i=1) &= P(t_i \leq T) \\ &= P(x_i'\delta + \varepsilon_i \leq T) \\ &= P\left(\varepsilon_i \leq T - x_i'\delta\right) \\ &= P\left(\frac{\varepsilon_i}{\sigma} \leq \frac{T - x_i'\delta}{\sigma}\right) \\ &= \Phi\left(\frac{T - x_i'\delta}{\sigma}\right)\\ &=\mu \end{align*}
   $$
   即：$\Phi^{-1}(\mu) = \beta_0 + x_i'\beta$。

2. 学习了log-binomial模型，它的连接函数是$\log(\cdot)$，它能直接估计出RR值（RR=$e^\beta$）。当时患病率大于10%时，OR不能很好得代表RR值，此时估计RR应该用log-binomial回归；

3. 学完了广义线性模型；

4. 汇报了第一周看的文献；

5. 学完了概率论第八章《极限定理》，简单了解了一致收敛的概念，简单学习了关于概率的各种不等式；

### 第四周（6.23-6.29）

1. 学完了概率论第九章《概率论的其他课题》，学习了泊松过程、马尔科夫链、不确定性和熵，学到了两个公式，其中 $H$ 表示熵：
   $$
   H(X,Y)=H(X)+H(Y|X)\\
   H(Y|X)\leqslant H(Y)
   $$
   第二个公式中等号成立的条件是 $X$ 和 $Y$ 独立，它的含义是对于随机向量 $(X, Y)$，当其中一个被观测时，另一个随机变量的熵会减小。可以简单理解为如果 $X$ 和 $Y$ 有一定的相关性（不一定是线性相关），当其中一个被观测到了，相关性的那部分也会被观测到，因此不确定性会减少；而当二者独立时，不存在相关性，不确定性也不会减少；

2. 学完了概率论第十章《模拟》，重点学习了方差缩减技术；
