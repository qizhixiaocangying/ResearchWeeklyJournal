## 6.30-7.27 科研周记

### 第一周（6.30-7.6）

1. 了解了广义交叉验证，它是留一交叉验证在线性模型中的近似实现方式，不需要多次划分样本；

2. 学到了矩阵的 Frobenius 范数的平方等于矩阵 $A^TA$ 的迹，Frobenius 范数的平方等于所有元素平方和；

3. 学习了加性模型和广义加性模型，重点学习了惩罚样条，它的本质是多项式回归（或者说基函数回归），同时在最小二乘法中加一个关于多项式函数二阶导的惩罚项，保证足够平滑（二阶导越大变化越剧烈），推导后的系数其实就是加权岭回归，权重由选择的基函数确定。惩罚样条是一种**估计方法**，而非一类特定的样条曲线；

4. 加性模型的“加性”指的是可加性假设，它是指对于一般的非参数模型：$y_i = f(x_{1,i}, x_{2,i}, \dots, x_{p,i}) + \varepsilon_i, \quad \varepsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)$   假设 $f$ 具有可加性，则模型可以写成：$y_i = \beta_1 f_1(x_{1,i}) + \beta_2 f_2(x_{2,i}) + \dots + \beta_p f_p(x_{p,i}) + \varepsilon_i$，这样就能得到更具可解释性的系数；

5. 学习了 $\text{Type}$ $\text{I}$、$\text{II}$、$\text{III}$​ 平方和，它们是回归分析中求主效应和交互效应的不同方法，原理是用纳入不同项的模型残差相减得到相应的效应，下表是他们的含义：

   | 平方和类型 | $\text{SS}_A$ 的对比模型            | $\text{SS}_B$ 的对比模型            | $\text{SS}_{AB}$ 的对比模型     | 核心特点                      |
   | ---------- | ------------------------------------- | ------------------------------------- | --------------------------------- | ----------------------------- |
   | **1 型**   | $M_0$ vs $M_A$                    | $M_A$ vs $M_{AB}$                 | $M_{AB}$ vs $M_{\text{full}}$ | 依赖效应顺序，先算的效应优先  |
   | **2 型**   | $M_B$ vs $M_{AB}$                 | $M_A$ vs $M_{AB}$                 | $M_{AB}$ vs $M_{\text{full}}$ | 主效应仅排除其他主效应        |
   | **3 型**   | $M_{B+AB}^*$ vs $M_{\text{full}}$ | $M_{A+AB}^*$ vs $M_{\text{full}}$ | $M_{AB}$ vs $M_{\text{full}}$ | 排除所有其他效应（主 + 交互） |

   它们可以用于组别不平衡时的多因素方差分析，这时组间方差分解不再正交，不能完全分离出主效应和交互效应，举个例子：**A₁组**：8 个数据，3 个来自 B₁，5 个来自 B₂；**A₂组**：9 个数据，4 个来自 B₁，5 个来自 B₂；**A₃组**：7 个数据，3 个来自 B₁，4 个来自 B₂，这三个A组的B类别不平衡，不能完全消除B的效应。这三个平方和在平衡数据时是等价的，数据不平衡时，三种平方和的差异是对变异重叠的不同处理方式，但均无法绕过数据本身的局限性实现 “完全分离”。3型更适合不平衡、含交互、验证独立效应的场景；

6. 学习了permutation test，它的思路是多次排列样本，对每次重新排列的样本构造统计量，然后得到统计量的抽样分布，从而根据原始排列的统计量的位置进行假设检验。置换检验的关键在于**识别零假设下数据的哪一部分属性是“可交换”的（即可以随机重排而不改变零假设的性质）**，然后通过随机重排（置换）这些可交换的部分来模拟零假设成立时的数据行为，构建零分布。分组标签只是实现可交换性的一种常见载体，但不是唯一载体。

7. 简单了解了群的定义和变换群的定义，群就是满足一定性质的集合加二元运算；

8. 学习了EM算法和高斯混合模型。EM算法是一种参数估计方法，它的核心就是以下公式，其中E表示求期望，M表示求最大：
   $$
   \theta^{(t+1)} = \arg\max_{\theta} \int_{\mathcal{Z}} \log p(X, Z \mid \theta) \cdot p(Z \mid X, \theta^{(t)}) \, dz = \arg\max_{\theta} \mathbb{E}_{Z \mid X, \theta^{(t)}} \left[ \log p(X, Z \mid \theta) \right]
   $$
   它的原理是引入一个隐变量，通过隐变量简化似然函数，最终迭代收敛。高斯混合模型是多个高斯分布的加权平均，它利用EM算法估计参数，它可以用于（软）聚类，能得到数据的概率分布；


### 第二周（7.7-7.13）

1. 简单了解了M估计；
1. 有了改进阈值最小二乘法的思路，迭代重加权最小二乘法不可行，因为以前的损失函数不满足M估计的要求，新思路是：修改临床目的为临床等效性预测，采用连续的临床等效性损失函数，证明损失函数的凸性，用BFGS优化，并通过理论和实验证明收敛性；


### 第三周（7.14-7.20）

1. 做完了改进后阈值最小二乘法的实验部分，但是结果不太好，优化下面这个损失函数没法保证修改的MAE和MSE小于普通线性回归，可能需要修改残差分布和损失函数：
   $$
   L_{CEL}(e_i) =  \begin{cases}  0 & |e_i| \leq \delta \\ \frac{1}{2} (|e_i| - \delta)^2 & |e_i| > \delta  \end{cases}
   $$

2. 明白了优化上面那个损失函数为什么不能保证修改后的MSE最小，因为超出阈值的部分可以分解，只有分解后的第一项是MSE，所以最小化整体不能保证超出部分的MSE最小。同理，如果超出部分的损失函数改为 $|e_i|-\delta$，同样不能保证超出部分的MAE最小，因为后面一项 $\delta$ 并不是常数，它跟超出阈值部分的数量有关；

3. 改回原来的损失函数：$L_{CEL}(e_i) =  \begin{cases}  0 & |e_i| \leq \delta \\ \frac{1}{2} e_i^2 & |e_i| > \delta  \end{cases}$ ，并用BFGS优化后能保证比线性回归更优，但是这个损失函数性质很差，需要考虑如何证明其收敛性；

4. 



### 第四周（7.21-7.27）

1. 
